{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e3e04-a15a-4a97-bc1b-3437a892ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "print(\"\")\n",
    "print(\"+\"*shutil.get_terminal_size().columns)\n",
    "print(\"Hierarchical Bayesian Analysis of Kepler Eccentricities\")\n",
    "print(\"Initialized {0}\".format(datetime.now().strftime(\"%d-%b-%Y at %H:%M:%S\")))\n",
    "print(\"+\"*shutil.get_terminal_size().columns)\n",
    "print(\"\")\n",
    "\n",
    "# track date\n",
    "YYYYMMDD = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# start program timer\n",
    "global_start_time = timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7e5116-85a2-416d-bdad-d4fd50a1ab40",
   "metadata": {},
   "source": [
    "## Parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc145e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANUAL DEVELOPMENT INPUTS START\n",
    "\n",
    "PROJECT_DIR  = '/Users/research/projects/kepler-ecc-rp/'\n",
    "#DATA_SOURCE  = 'DR25'\n",
    "#DATA_DIR     = '/Users/research/data/DR25_chains/'\n",
    "DATA_SOURCE  = 'ALDERAAN'\n",
    "DATA_DIR     = '/Users/research/projects/alderaan/Results/ecc-all-LC-final/'\n",
    "RUN_ID       = 'develop'\n",
    "DISTRIBUTION = 'histogram'\n",
    "NSAMP        = 1000\n",
    "BOOTSTRAP    = 'none'\n",
    "MULTIPLICITY = (1,1)\n",
    "PER_LIM      = (1,100)\n",
    "RAD_TYPE     = 'rp'\n",
    "RAD_LIM      = (2.0,2.0)\n",
    "RAD_FWHM     = 0.2         # fractional full width half max of Gaussian bins (if appliable)\n",
    "MSTAR_LIM    = (0.,10.)\n",
    "RSTAR_LIM    = (0.7,1.4)\n",
    "FEH_LIM      = (-0.5,0.5)\n",
    "TEFF_LIM     = (4700,6500)\n",
    "AGE_LIM      = (0,14)\n",
    "E_DETPRIOR   = True\n",
    "B_DETPRIOR   = False\n",
    "B_CUT        = 0.05\n",
    "\n",
    "# set number of bins\n",
    "if DISTRIBUTION == 'histogram':\n",
    "    NBIN = 25\n",
    "else:\n",
    "    NBIN = 100\n",
    "\n",
    "# auto-generate random run_id\n",
    "ascii = string.ascii_letters + string.digits\n",
    "for i in range(8):\n",
    "    RUN_ID += random.choice(ascii)\n",
    "\n",
    "DO_PLOT = True\n",
    "\n",
    "# MANUAL DEVELOPMENT INPUTS END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643291b5-6dd0-4abb-87ad-63fb54ac64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # read the arguments\n",
    "    parser = argparse.ArgumentParser(description=\"Inputs for Kepler hierarchical eccentricities analysis\")\n",
    "    parser.add_argument(\"--project_dir\", default='/data/user/gjgilbert/projects/kepler-ecc-rp/', type=str, required=False, \\\n",
    "                        help=\"project directory for storing inputs and outputs\")\n",
    "    parser.add_argument(\"--data_dir\", default=None, type=str, required=True, \\\n",
    "                        help=\"data directory for accessing DR25 posterior chains\")\n",
    "    parser.add_argument(\"--data_source\", default=None, type=str, required=True, \\\n",
    "                        help=\"can be 'DR25' or 'ALDERAAN'\")\n",
    "    parser.add_argument(\"--run_id\", default=None, type=str, required=True, \\\n",
    "                        help=\"unique run identifier\")\n",
    "    parser.add_argument(\"--distribution\", default='empirical', type=str, required=False, \\\n",
    "                        help=\"probability density function to use; can be 'histogram', 'empirical', 'beta', 'halfnormal', 'expon', 'rayleigh', 'invpareto'\")\n",
    "    parser.add_argument(\"--nsamp\", default=1000, type=int, required=False, \\\n",
    "                        help=\"number of (re)samples to feed into hbayes model\")\n",
    "    parser.add_argument(\"--nbin\", default=100, type=int, required=False, \\\n",
    "                        help=\"number of bins for probability density function'\")\n",
    "    parser.add_argument(\"--bootstrap\", default='none', type=str, required=False, \\\n",
    "                        help=\"bootrap method to use, can be 'none', 'wlb', or an integer\")\n",
    "\n",
    "    parser.add_argument(\"--multiplicity\", default=(1,99), nargs=2, type=int, required=False, \\\n",
    "                        help=\"(lower,upper) limits on multiplicity\")\n",
    "    parser.add_argument(\"--per_lim\", default=(1,300), nargs=2, type=float, required=False, \\\n",
    "                        help=\"(lower,upper) limits on period\")\n",
    "    parser.add_argument(\"--rad_type\", default=None, type=str, required=True, \\\n",
    "                        help=\"radius type to use' can by 'rp', 'rp10', or 'rpadj'\")\n",
    "    parser.add_argument(\"--rad_lim\", default=None, nargs=2, type=float, required=True, \\\n",
    "                        help=\"(lower,upper) limits on radius; set lower=upper to use Gaussian binning\")\n",
    "    parser.add_argument(\"--rad_fwhm\", default=None, type=float, required=False, \\\n",
    "                        help=\"fractional FWHM on radius bins if using Gaussian binning'\")\n",
    "\n",
    "    parser.add_argument(\"--mstar_lim\", default=(0.,10.), nargs=2, type=float, required=False, \\\n",
    "                        help=\"(lower,upper) limits on stellar mass\")\n",
    "    parser.add_argument(\"--rstar_lim\", default=(0.7,1.4), nargs=2, type=float, required=False, \\\n",
    "                        help=\"(lower,upper) limits on stellar radius\")\n",
    "    parser.add_argument(\"--feh_lim\", default=(-0.6,0.6), nargs=2, type=float, required=False, \\\n",
    "                        help=\"(lower,upper) limits on stellar metallicity\")\n",
    "    parser.add_argument(\"--teff_lim\", default=(4700,6500), nargs=2, type=float, required=False, \\\n",
    "                        help=\"(lower,upper) limits on stellar effective temperature\")\n",
    "    parser.add_argument(\"--age_lim\", default=(0,14), nargs=2, type=float, required=False, \\\n",
    "                        help=\"(lower,upper) limits on stellar age\")\n",
    "\n",
    "    parser.add_argument(\"--e_detprior\", default=1, type=int, required=False, \\\n",
    "                        help=\"flag to use geometric eccentricity detection prior (1) or not (0)\")\n",
    "    parser.add_argument(\"--b_detprior\", default=0, type=int, required=False, \\\n",
    "                        help=\"flag to use impact parameter detection prior (1) or not (0)\")\n",
    "    parser.add_argument(\"--b_cut\", default=0.01, type=float, required=False, \\\n",
    "                        help=\"fraction of impact parameter samples which are allowed to be grazing\")\n",
    "\n",
    "    # parse the arguments\n",
    "    args = parser.parse_args()\n",
    "    PROJECT_DIR  = args.project_dir\n",
    "    DATA_DIR     = args.data_dir\n",
    "    DATA_SOURCE  = args.data_source\n",
    "    RUN_ID       = args.run_id\n",
    "    DISTRIBUTION = args.distribution\n",
    "    NSAMP        = args.nsamp\n",
    "    NBIN         = args.nbin\n",
    "    BOOTSTRAP    = args.bootstrap\n",
    "    MULTIPLICITY = args.multiplicity\n",
    "    PER_LIM      = args.per_lim\n",
    "    RAD_TYPE     = args.rad_type\n",
    "    RAD_LIM      = args.rad_lim\n",
    "    RAD_FWHM     = args.rad_fwhm\n",
    "    MSTAR_LIM    = args.mstar_lim\n",
    "    RSTAR_LIM    = args.rstar_lim\n",
    "    FEH_LIM      = args.feh_lim\n",
    "    TEFF_LIM     = args.teff_lim\n",
    "    AGE_LIM      = args.age_lim\n",
    "    E_DETPRIOR   = bool(args.e_detprior)\n",
    "    B_DETPRIOR   = bool(args.b_detprior)\n",
    "    B_CUT        = args.b_cut\n",
    "    DO_PLOT      = False\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    BOOTSTRAP = int(BOOTSTRAP)\n",
    "except ValueError:\n",
    "    pass\n",
    "    \n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, 'Results', YYYYMMDD)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(\" Distribution: {0}\".format(DISTRIBUTION))\n",
    "print(\"\")\n",
    "print(\"   npl = {0}-{1}\".format(MULTIPLICITY[0], MULTIPLICITY[1]))\n",
    "print(\"   P   = {0}-{1}\".format(PER_LIM[0], PER_LIM[1]))\n",
    "print(\"   {0} = {1}-{2}\".format(RAD_TYPE, RAD_LIM[0], RAD_LIM[1]))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f98707-cb03-452a-99ac-0356b9fa2e2b",
   "metadata": {},
   "source": [
    "## Import packages and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142e684-6b19-4d99-b712-9f96b981f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.constants as apc\n",
    "from   astropy.io import fits\n",
    "from   copy import deepcopy\n",
    "import diptest\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "from   matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   scipy.interpolate import interp1d, UnivariateSpline, CubicSpline\n",
    "from   scipy import stats\n",
    "from   scipy.special import erf, erfinv, logsumexp\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import aesara_theano_fallback.tensor as T\n",
    "from   aesara_theano_fallback import aesara as theano\n",
    "from   celerite2.theano import GaussianProcess\n",
    "from   celerite2.theano import terms as GPterms\n",
    "import pymc3 as pm\n",
    "import pymc3_ext as pmx\n",
    "\n",
    "sys.path.append(PROJECT_DIR)\n",
    "from utils.astro import calc_T14_circ, calc_sma, calc_aRs, jacobian, detection_prior, duration_ratio\n",
    "from utils.distributions import BetaDistLogPDF, NormDistLogPDF, ExponDistLogPDF, RayleighDistLogPDF, InvParetoDistLogPDF\n",
    "from utils.eccsamp import imp_sample_rhostar\n",
    "from utils.io import load_dr25_data_from_hdf5\n",
    "from utils.models import build_simple_model, build_multilevel_model\n",
    "from utils.stats import weighted_percentile, draw_random_samples, gelman_rubin\n",
    "\n",
    "sys.path.append('/Users/research/projects/alderaan/')\n",
    "sys.path.append('/data/user/gjgilbert/projects/alderaan/')\n",
    "from alderaan.Results import Results\n",
    "\n",
    "pi = np.pi\n",
    "\n",
    "RSAU = (apc.R_sun/apc.au).value                                 # solar radius [AU]\n",
    "RSRE = (apc.R_sun/apc.R_earth).value                            # R_sun/R_earth\n",
    "RHOSUN_GCM3 = (3*apc.M_sun/(4*pi*apc.R_sun**3)).value/1000      # solar density [g/cm^3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN SCRIPT BEGINS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af8c1e-0e90-40d4-910c-8740bddb8128",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d168011-3a48-4675-ab26-2587de5c5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1af56-2e9a-4e84-be89-afc11d205c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SOURCE == 'Kepler':\n",
    "    CATALOG = os.path.join(PROJECT_DIR, 'Catalogs/kepler_dr25_gaia_dr2_crossmatch.csv')\n",
    "elif DATA_SOURCE == 'ALDERAAN':    \n",
    "    CATALOG = os.path.join(PROJECT_DIR, 'Catalogs/kepler_dr25_gaia_dr2_crossmatch.csv')\n",
    "elif DATA_SOURCE == 'ALDERAAN-INJECTION':\n",
    "    CATALOG = os.path.join(DATA_DIR, '{0}.csv'.format(DATA_DIR[DATA_DIR.find('Results')+8:-1]))\n",
    "else:\n",
    "    raise ValueError(\"Unsuported data source\")\n",
    "\n",
    "catalog = pd.read_csv(CATALOG, index_col=0)\n",
    "\n",
    "# make injection catalog look like real catalog\n",
    "if DATA_SOURCE == 'ALDERAAN-INJECTION':\n",
    "    n = len(catalog.koi_id)\n",
    "    \n",
    "    catalog.columns = map(str.lower, catalog.columns)\n",
    "    \n",
    "    # track ground-truth injected values\n",
    "    catalog['true_rp'] = np.copy(catalog.ror * catalog.rstar * RSRE)\n",
    "    catalog['true_rstar'] = np.copy(catalog.rstar)\n",
    "    catalog['true_rhostar'] = np.copy(10**catalog.logrho)\n",
    "    \n",
    "    # add measurement error to stellar properties\n",
    "    catalog['rstar'] = catalog.rstar + 0.04*stats.truncnorm(a=-2, b=2, loc=0,scale=1).rvs(n)*catalog.rstar\n",
    "    catalog['rstar_err1'] = 0.04*catalog.rstar\n",
    "    catalog['rstar_err2'] = -0.04*catalog.rstar\n",
    "    catalog['rhostar'] = 10**catalog.logrho + 0.13*stats.truncnorm(a=-2, b=2, loc=0,scale=1).rvs(n)*(10**catalog.logrho)\n",
    "    catalog['rhostar_err1'] = 0.13*catalog.rhostar\n",
    "    catalog['rhostar_err2'] = -0.13*catalog.rhostar\n",
    "     \n",
    "    catalog['mstar'] = catalog.rhostar * catalog.rstar**3\n",
    "    catalog['age'] = 5.0*np.ones(n)\n",
    "\n",
    "    catalog['fpp'] = np.zeros(n)\n",
    "    catalog['rcf'] = np.ones(n)\n",
    "    catalog['ruwe'] = np.ones(n)\n",
    "    \n",
    "    catalog['disposition'] = ['CONFIRMED']*len(catalog['koi_id'])\n",
    "    catalog['rp'] = catalog.ror * catalog.rstar * RSRE\n",
    "    catalog['rp_err'] = 0.1*catalog.rp\n",
    "\n",
    "    planet_name = np.array(catalog.koi_id.values, dtype='U9')\n",
    "    for i, p in enumerate(planet_name):\n",
    "        planet_name[i] = planet_name[i] + '.01'\n",
    "    catalog['planet_name'] = planet_name\n",
    "        \n",
    "\n",
    "# hard-code period and radius limits\n",
    "use  = (catalog.period > 1) * (catalog.period < 300)\n",
    "use *= (catalog.rp > 0) * (catalog.rp < 16)\n",
    "\n",
    "# remove likely false positives\n",
    "use *= (catalog.fpp < 0.1) + (catalog.disposition == 'CONFIRMED')\n",
    "\n",
    "# clean up stellar sample\n",
    "use *= catalog.logg > 4.0                                   # surface gravity as proxy for main sequence\n",
    "use *= ((catalog.rcf - 1) < 0.05) + np.isnan(catalog.rcf)   # radius correction factor (Furlan+ 2017)\n",
    "use *= catalog.ruwe < 1.4                                   # Gaia RUWE\n",
    "\n",
    "Rstar = catalog.rstar\n",
    "Rstar_err = np.sqrt(catalog.rstar_err1**2 + catalog.rstar_err2**2)/np.sqrt(2)\n",
    "\n",
    "use *= Rstar_err/Rstar < 0.2\n",
    "\n",
    "# slice subpopulation\n",
    "use *= ((catalog.npl >= MULTIPLICITY[0]) &\n",
    "        (catalog.npl <= MULTIPLICITY[1]) &\n",
    "        (catalog.period >= PER_LIM[0]) &\n",
    "        (catalog.period <= PER_LIM[1]) &\n",
    "        (catalog.mstar >= MSTAR_LIM[0]) &\n",
    "        (catalog.mstar <= MSTAR_LIM[1]) &\n",
    "        (catalog.rstar >= RSTAR_LIM[0]) &\n",
    "        (catalog.rstar <= RSTAR_LIM[1]) &\n",
    "        (catalog.feh >= FEH_LIM[0]) &\n",
    "        (catalog.feh <= FEH_LIM[1]) &\n",
    "        (catalog.teff >= TEFF_LIM[0]) &\n",
    "        (catalog.teff <= TEFF_LIM[1]) &\n",
    "        (catalog.age >= AGE_LIM[0]) &\n",
    "        (catalog.age <= AGE_LIM[1])\n",
    "       )\n",
    "\n",
    "\n",
    "# tophat binning (with 30% buffer)\n",
    "if RAD_LIM[0] != RAD_LIM[1]:\n",
    "    use *= ((catalog[RAD_TYPE] >= 0.3*RAD_LIM[0]) & (catalog[RAD_TYPE] <= 1.3*RAD_LIM[1]))\n",
    "    \n",
    "\n",
    "# Gaussian binning (5-sigma)\n",
    "else:\n",
    "    # grab objects within 3-sigma of bin center\n",
    "    rad_center = RAD_LIM[0]\n",
    "    rad_sigma  = np.sqrt(catalog[RAD_TYPE+'_err']**2 + (RAD_FWHM/2.355*rad_center)**2)\n",
    "    rad_low    = np.max([0.1, np.min(rad_center-5*rad_sigma)])\n",
    "    rad_high   = np.min([20., np.max(rad_center+5*rad_sigma)])\n",
    "\n",
    "    use *= np.abs(catalog[RAD_TYPE] - rad_center)/rad_sigma < 5.0\n",
    "    \n",
    "    \n",
    "# update targets and catalog\n",
    "catalog = catalog.loc[use].reset_index(drop=True)\n",
    "targets = np.array(catalog.planet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47e269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a hacky bit of code to simulate a pristine radius gap\n",
    "GAP_WIDTH = None\n",
    "\n",
    "if GAP_WIDTH is not None:\n",
    "    use = (catalog.true_rp < 1.84/(1+GAP_WIDTH/2)) + (catalog.true_rp > 1.84*(1+GAP_WIDTH/2))\n",
    "\n",
    "    catalog = catalog.loc[use].reset_index(drop=True)\n",
    "    targets = np.array(catalog.planet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46a330-0457-4153-8c27-c44b48953dd7",
   "metadata": {},
   "source": [
    "#### Load posterior chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e9f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_planet_koi_from_period(star_koi, P_samp, catalog):\n",
    "    P_all = catalog.loc[catalog.koi_id==star_koi, 'period'].values\n",
    "    P_cat = P_all[np.argmin(np.abs(P_all - P_samp))]\n",
    "        \n",
    "    return catalog.loc[catalog.period==P_cat, 'planet_name'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3571fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_index_from_planet_koi(planet_koi, results, catalog):\n",
    "    periods = np.zeros(results.npl)\n",
    "\n",
    "    for n in range(results.npl):\n",
    "        periods[n] = np.median(results.samples(n).PERIOD)\n",
    "    \n",
    "    return np.argmin(np.abs(periods - catalog.loc[catalog.planet_name==planet_koi, 'period'].values))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643e265-acf2-491c-b89a-aab0d6c1adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains  = {}\n",
    "failure = []\n",
    "\n",
    "# read in the data\n",
    "if DATA_SOURCE == 'DR25':\n",
    "    CHAINS = os.path.join(DATA_DIR, 'dr25-chains_trimmed-thinned.hdf')\n",
    "\n",
    "    for i, t in enumerate(targets):\n",
    "        try:\n",
    "            chains[t] = pd.DataFrame(load_dr25_data_from_hdf5(CHAINS, t))\n",
    "            chains[t]['DUR14'] = calc_T14_circ(chains[t].PERIOD, chains[t].ROR, chains[t].IMPACT, chains[t].RHOTILDE)\n",
    "\n",
    "            if np.any(chains[t].values < 0):\n",
    "                raise ValueError(\"Negative values in posterior chain\")\n",
    "            if np.sum(np.isnan(chains[t].values)) > 0:\n",
    "                raise ValueError(\"NaN values in posterior chain\")\n",
    "\n",
    "        except:\n",
    "            warnings.warn(\"{0} failed to load\".format(t))\n",
    "            failure.append(t)\n",
    "\n",
    "        \n",
    "# Alderaan analysis only configured for single-transiting systems\n",
    "elif DATA_SOURCE == 'ALDERAAN':\n",
    "    files = np.sort(glob.glob(os.path.join(DATA_DIR, '*/*results.fits')))\n",
    "    \n",
    "    for i, t in enumerate(targets):\n",
    "        try:\n",
    "            results = Results(t[:-3], DATA_DIR)\n",
    "            n = infer_index_from_planet_koi(t, results, catalog)\n",
    "\n",
    "            chains[t] = results.samples(n).sample(n=8000, replace=True, weights=results.posteriors.weights(), ignore_index=True)\n",
    "            chains[t] = chains[t].drop(columns='LN_WT')\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            warnings.warn(\"{0} failed to load\".format(t))\n",
    "            failure.append(t)\n",
    "\n",
    "\n",
    "elif DATA_SOURCE == 'ALDERAAN-INJECTION':\n",
    "    files = np.sort(glob.glob(os.path.join(DATA_DIR, '*/*results.fits')))\n",
    "    \n",
    "    for i, t in enumerate(targets):\n",
    "        try:\n",
    "            \n",
    "            results = Results('S'+t[1:-3], DATA_DIR)\n",
    "            n = infer_index_from_planet_koi(t, results, catalog)\n",
    "\n",
    "            chains[t] = results.samples(n).sample(n=8000, replace=True, weights=results.posteriors.weights(), ignore_index=True)\n",
    "            chains[t] = chains[t].drop(columns='LN_WT')\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            warnings.warn(\"{0} failed to load\".format(t))\n",
    "            failure.append(t)            \n",
    "            \n",
    "else:\n",
    "    raise ValueError(\"Data source must be either 'DR25'or 'ALDERAAN' or 'ALDERAAN-INJECTION'\")\n",
    "    \n",
    "\n",
    "# update targets and catalog\n",
    "targets = list(np.array(targets)[~np.isin(targets,failure)])\n",
    "catalog = catalog.loc[np.isin(catalog.planet_name, targets)].reset_index(drop=True)\n",
    "\n",
    "print(\"{0} targets loaded\".format(len(targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e56394",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0} planets\".format(len(targets)))\n",
    "print(\"{0} stars\".format(len(np.unique(catalog.koi_id))))\n",
    "print(\"{0} singles\".format(np.sum(catalog.npl == 1)))\n",
    "print(\"{0} multis\".format(np.sum(catalog.npl > 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d0ee2",
   "metadata": {},
   "source": [
    "#### Update transit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_keys = 'period epoch ror duration impact'.split()\n",
    "samples_keys = 'PERIOD T0 ROR DUR14 IMPACT'.split()\n",
    "\n",
    "for i, t in enumerate(targets):\n",
    "    use = catalog.planet_name == t\n",
    "    samples = chains[t]\n",
    "    \n",
    "    for j, ck in enumerate(catalog_keys):\n",
    "        sk = samples_keys[j]\n",
    "\n",
    "        catalog.loc[use, ck] = np.median(samples[sk])\n",
    "        catalog.loc[use, ck+'_err1'] = np.percentile(samples[sk], 84) - catalog.loc[use, ck]\n",
    "        catalog.loc[use, ck+'_err2'] = np.percentile(samples[sk], 16) - catalog.loc[use, ck]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70351a",
   "metadata": {},
   "source": [
    "#### Calculate self-consistent planet radii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "ror = catalog.ror\n",
    "ror_err = np.sqrt(catalog.ror_err1**2 + catalog.ror_err2**2)/np.sqrt(2)\n",
    "\n",
    "Rstar = catalog.rstar\n",
    "Rstar_err = np.sqrt(catalog.rstar_err1**2 + catalog.rstar_err2**2)/np.sqrt(2)\n",
    "\n",
    "# radius gap location from Petigura+2022; R = R0*(P/10)^y, R0 = 1.84 +/- 0.03, y = 0.11 +/- 0.02\n",
    "catalog['rgap'] = np.array(1.84*(catalog.period/10)**-0.11)\n",
    "catalog['rgap_err'] = catalog.rgap * np.sqrt( 0.02**2*np.log(catalog.period/10)**2 + (0.03/1.84)**2)\n",
    "\n",
    "# physical planet radius\n",
    "catalog['rp'] = np.array(ror*Rstar*RSRE)\n",
    "catalog['rp_err'] = np.array(catalog.rp * np.sqrt((ror_err/ror)**2 + (Rstar_err/Rstar)**2))\n",
    "\n",
    "# radius corrected to P=10 days (see Ho & Van Eylen 2023); equivalent to using diagonal bins\n",
    "catalog['rp10'] = np.exp(np.log(catalog.rp) - np.log(catalog.rgap) + np.log(1.84))\n",
    "catalog['rp10_err'] = np.sqrt(catalog.rp_err**2 + catalog.rgap_err**2)\n",
    "\n",
    "# radius adjusted for super-Earths and sub-Neptunes only\n",
    "rp = catalog.rp\n",
    "rgap = catalog.rgap\n",
    "rp_adj = np.array(rp)\n",
    "\n",
    "rp_lower_lim = 1.0\n",
    "rp_gap10_loc = 1.84\n",
    "rp_giant_lim = 4.0\n",
    "\n",
    "SE = (rp >= rp_lower_lim)*(rp < rgap)\n",
    "SN = (rp >= rgap)*(rp < rp_giant_lim)\n",
    "GP = (rp >= rp_giant_lim)\n",
    "\n",
    "rp_adj[SE] = ((rp - rp_lower_lim)/(rgap - rp_lower_lim) * (rp_gap10_loc - rp_lower_lim) + rp_lower_lim)[SE]\n",
    "rp_adj[SN] = ((rp - rgap)/(rp_giant_lim - rgap) * (rp_giant_lim - rp_gap10_loc) + rp_gap10_loc)[SN]\n",
    "rp_adj[GP] = rp[GP]\n",
    "\n",
    "rp_adj_err = np.copy(catalog['rp_err'])\n",
    "rp_adj_err[SE+SN] = np.array(catalog['rp10_err'])[SE+SN]\n",
    "\n",
    "catalog['rpadj'] = rp_adj\n",
    "catalog['rpadj_err'] = rp_adj_err\n",
    "\n",
    "\n",
    "# require better than 20% precision on radius\n",
    "use = catalog.rp_err/catalog.rp < 0.2\n",
    "\n",
    "catalog = catalog.loc[use].reset_index(drop=True)\n",
    "targets = np.array(catalog.planet_name)\n",
    "\n",
    "print(\"{0} targets found with uncertain planetary radii (> 20%)\".format(np.sum(~use)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba122df",
   "metadata": {},
   "source": [
    "#### Flag unreliable posterior chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6086512",
   "metadata": {},
   "outputs": [],
   "source": [
    "failure = []\n",
    "\n",
    "if DATA_SOURCE == 'DR25':\n",
    "    for i, t in enumerate(targets):\n",
    "        # remove grazing transits\n",
    "        if np.any(chains[t].IMPACT.values > 1 - chains[t].ROR.values):\n",
    "            failure.append(t)\n",
    "\n",
    "        # eliminate NaN and zero-valued chains\n",
    "        if np.any(chains[t].values < 0):\n",
    "            failure.append(t)\n",
    "        if np.sum(np.isnan(chains[t].values)) > 0:\n",
    "            failure.append(t)\n",
    "\n",
    "        # check Gelman-Rubin convergence statistic\n",
    "        for k in chains[t].keys():\n",
    "            Rhat = gelman_rubin(chains[t][k].values.reshape(4,-1))\n",
    "            if Rhat > 1.05:\n",
    "                failure.append(t)\n",
    "\n",
    "        # check Hartigan dip test for multimodality\n",
    "        for k in chains[t].keys():\n",
    "            dip, pval = diptest.diptest(chains[t][k].values)\n",
    "            if pval < 0.05:            \n",
    "                failure.append(t)\n",
    "\n",
    "                \n",
    "grazing_fraction = []                \n",
    "                \n",
    "if DATA_SOURCE == 'ALDERAAN' or 'ALDERAAN-INJECTION':\n",
    "    for i, t in enumerate(targets):\n",
    "        # remove grazing transits\n",
    "        grazing = chains[t].IMPACT.values > 1 - chains[t].ROR.values\n",
    "        \n",
    "        grazing_fraction.append(np.sum(grazing)/len(grazing))\n",
    "\n",
    "        if grazing_fraction[i] > B_CUT:\n",
    "            failure.append(t)\n",
    "\n",
    "# update targets and catalog\n",
    "targets = list(np.array(targets)[~np.isin(targets,failure)])\n",
    "catalog = catalog.loc[np.isin(catalog.planet_name, targets)].reset_index(drop=True)\n",
    "\n",
    "print(\"{0} targets found with unreliable chains\".format(len(np.unique(failure))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf = np.array(grazing_fraction)\n",
    "\n",
    "if DO_PLOT:\n",
    "    plt.figure()\n",
    "    plt.hist(gf[gf > 0], color='lightgrey', bins=np.linspace(0,0.2,41))\n",
    "    plt.hist(gf[gf > 0], color='k', histtype='step', bins=np.linspace(0,0.2,41))\n",
    "    plt.axvline(0.01, color='r', ls='--')\n",
    "    plt.axvline(0.05, color='r', ls=':')\n",
    "    plt.xticks(np.linspace(0,0.2,11))\n",
    "    plt.xlabel(\"fraction of samples with $b > 1 - R_p/R_s$\", fontsize=14)\n",
    "    plt.ylabel(\"number of planets\", fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a1a4e",
   "metadata": {},
   "source": [
    "#### Grab stellar densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "density = {}\n",
    "failure = []\n",
    "\n",
    "for i, t in enumerate(targets):\n",
    "    try:\n",
    "        use = catalog.planet_name == t\n",
    "        \n",
    "        rho_mu = catalog.loc[use, 'rhostar'].iloc[0]\n",
    "        rho_err1 = np.abs(catalog.loc[use, 'rhostar_err1'].iloc[0])\n",
    "        rho_err2 = np.abs(catalog.loc[use, 'rhostar_err2'].iloc[0])\n",
    "\n",
    "        # don't use highly asymmetric density constraints\n",
    "        if np.abs(rho_err1-rho_err2)/(0.5*(rho_err1+rho_err2)) > 0.30:\n",
    "            failure.append(t)\n",
    "        else:\n",
    "            density[t] = rho_mu, np.sqrt(rho_err1**2 + rho_err2**2)/np.sqrt(2)\n",
    "    \n",
    "    except:\n",
    "        warnings.warn(\"{0} has mising or unreliable density\".format(t))\n",
    "        failure.append(t)\n",
    "\n",
    "        \n",
    "# update targets and catalog\n",
    "targets = list(np.array(targets)[~np.isin(targets,failure)])\n",
    "catalog = catalog.loc[np.isin(catalog.planet_name, targets)].reset_index(drop=True)\n",
    "\n",
    "print(\"{0} targets found with missing or unreliable densities\".format(len(np.unique(failure))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a2329",
   "metadata": {},
   "source": [
    "## Slice subpopulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e08af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tophat binning\n",
    "if RAD_LIM[0] != RAD_LIM[1]:\n",
    "    use = (catalog[RAD_TYPE] >= RAD_LIM[0]) & (catalog[RAD_TYPE] <= RAD_LIM[1])\n",
    "    \n",
    "    catalog = catalog.loc[use].reset_index(drop=True)\n",
    "    targets = np.array(catalog.planet_name)\n",
    "\n",
    "\n",
    "# Gaussian binning (3 sigma)\n",
    "else:\n",
    "    rad_center = RAD_LIM[0]\n",
    "    rad_sigma  = np.sqrt(catalog[RAD_TYPE+'_err']**2 + (RAD_FWHM/2.355*rad_center)**2)\n",
    "    rad_low    = np.max([0.1, np.min(rad_center-3*rad_sigma)])\n",
    "    rad_high   = np.min([20., np.max(rad_center+3*rad_sigma)])\n",
    "\n",
    "    use = np.abs(catalog[RAD_TYPE] - rad_center)/rad_sigma < 3.0\n",
    "    \n",
    "    catalog = catalog.loc[use].reset_index(drop=True)\n",
    "    targets = np.array(catalog.planet_name)\n",
    "    \n",
    "\n",
    "# ensure radius adjustment is valid\n",
    "if RAD_TYPE == 'rp10':\n",
    "    use = (catalog.rp < 4.0) * (catalog.rp > 1.0)\n",
    "    \n",
    "    if np.sum(use) == 0:\n",
    "        raise ValueError(\"Rp10 correction is only valid for planets between 1-4 Earth-radii\")\n",
    "        \n",
    "    catalog = catalog.loc[use].reset_index(drop=True)\n",
    "    targets = np.array(catalog.planet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98d63b-692a-42f8-8d3e-f7502c40fefc",
   "metadata": {},
   "source": [
    "## Importance sample $\\{e,\\omega,\\rho_\\star\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebaa4d-9a82-438d-ac5f-c4614a77a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importance sampling...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88ba1c4-fb21-4dca-b286-0b0ce2d939a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "failure = []\n",
    "\n",
    "for i, t in enumerate(targets):\n",
    "    try:\n",
    "        # true stellar density (tuple) in g/cm3\n",
    "        rho_true = 1.41*density[t][0], 1.41*density[t][1]\n",
    "\n",
    "        w, d = imp_sample_rhostar(chains[t], rho_true, ew_obs_prior=False, upsample=500)\n",
    "        d = d.sample(n=NSAMP, replace=True, weights=w, ignore_index=True)\n",
    "\n",
    "        if DATA_SOURCE == 'DR25':\n",
    "            J = jacobian(d.PERIOD, d.ROR, d.IMPACT, d.DUR14)\n",
    "            d = d.sample(n=NSAMP, replace=True, weights=1/np.abs(J), ignore_index=True)\n",
    "\n",
    "        d = d.drop(columns = 'WEIGHTS')\n",
    "        data[t] = d\n",
    "\n",
    "    except:\n",
    "        warnings.warn(\"{0} failed during sampling and will not be included in the analysis\".format(t))\n",
    "        failure.append(t)\n",
    "\n",
    "        \n",
    "    \n",
    "# update targets and catalog\n",
    "targets = list(np.array(targets)[~np.isin(targets,failure)])\n",
    "catalog = catalog.loc[np.isin(catalog.planet_name, targets)].reset_index(drop=True)\n",
    "\n",
    "print(\"{0} targets failed importance sampling routine\".format(len(np.unique(failure))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbff79-fe02-490c-81bf-7059e8779969",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PLOT:\n",
    "    sns.set_context('paper', font_scale=1.2)\n",
    "    \n",
    "    if len(targets) > 50:\n",
    "        targets_to_plot = np.random.choice(targets, size=50, replace=False)\n",
    "    else:\n",
    "        targets_to_plot = np.copy(targets)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    for i, t in enumerate(targets_to_plot):\n",
    "        e  = data[t].ECC.values\n",
    "        e_ = np.linspace(0,1,1000)\n",
    "    \n",
    "        kde_e = stats.gaussian_kde(np.hstack([-e,e]), bw_method=0.1)\n",
    "        pdf_e_ = 2*kde_e(e_)\n",
    "    \n",
    "        mode_e = e_[np.argmax(pdf_e_)]\n",
    "\n",
    "        my_color = \"grey\"\n",
    "        my_cmap  = LinearSegmentedColormap.from_list(\"my_cmap\", [my_color, \"k\"], N=50)\n",
    "    \n",
    "        plt.plot(e_, pdf_e_, c=my_cmap(mode_e), zorder=100*mode_e, lw=2)\n",
    "        \n",
    "    plt.xlim(0,1)\n",
    "    plt.xlabel(\"$e$\", fontsize=16)\n",
    "    plt.ylabel(\"samples density\", fontsize=16)\n",
    "    plt.ylim(-0.1,5)\n",
    "    plt.yticks([])\n",
    "    \n",
    "    #plt.text(0.98, 4.85, \"sub-Earths\", fontsize=16, color=my_color, ha='right', va='top')\n",
    "    #plt.savefig(os.path.join(PROJECT_DIR, \"Figures/ecc-posterior-kde-subearths.pdf\"), bbox_inches='tight')\n",
    "    #plt.text(0.98, 4.85, \"$R_p$ = {0:.2f}-{1:.2f}\".format(RAD_LIM[0],RAD_LIM[1]), fontsize=16, color=my_color, ha='right', va='top')\n",
    "    #plt.savefig(os.path.join(PROJECT_DIR, \"Figures/ecc-posterior-kde-{0:.2f}-{1:.2f}.pdf\".format(RAD_LIM[0],RAD_LIM[1])), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0aa5c",
   "metadata": {},
   "source": [
    "## Calculate bin weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83af88c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_weights = {}\n",
    "\n",
    "for i, t in enumerate(targets):\n",
    "    d = data[t]\n",
    "    \n",
    "    rgap = catalog.loc[catalog.planet_name == t, 'rgap'].iloc[0]\n",
    "    rstar = catalog.loc[catalog.planet_name == t, 'rstar'].iloc[0]\n",
    "    \n",
    "    rp_samples = d.ROR.values*rstar*RSRE\n",
    "    rp10_samples = np.exp(np.log(rp_samples) - np.log(rgap) + np.log(1.84))\n",
    "    rpadj_samples = np.array(rp_samples)\n",
    "\n",
    "    rp_lower_lim = 1.0\n",
    "    rp_gap10_loc = 1.84\n",
    "    rp_giant_lim = 4.0\n",
    "\n",
    "    SE = (rp_samples >= rp_lower_lim)*(rp_samples < rgap)\n",
    "    SN = (rp_samples >= rgap)*(rp_samples < rp_giant_lim)\n",
    "    GP = (rp_samples >= rp_giant_lim)\n",
    "\n",
    "    rpadj_samples[SE] = ((rp_samples - rp_lower_lim)/(rgap - rp_lower_lim) * (rp_gap10_loc - rp_lower_lim) + rp_lower_lim)[SE]\n",
    "    rpadj_samples[SN] = ((rp_samples - rgap)/(rp_giant_lim - rgap) * (rp_giant_lim - rp_gap10_loc) + rp_gap10_loc)[SN]\n",
    "    rpadj_samples[GP] = rp_samples[GP]\n",
    "        \n",
    "    \n",
    "    if RAD_LIM[0] != RAD_LIM[1]:\n",
    "        sample_weights[t] = np.ones(NSAMP, dtype='float')/NSAMP\n",
    "\n",
    "    else:\n",
    "        rad_center = RAD_LIM[0]\n",
    "        rad_sigma  = RAD_FWHM/2.355*rad_center\n",
    "\n",
    "        if RAD_TYPE == 'rp':\n",
    "            w_ = stats.norm(rad_center, rad_sigma).pdf(rp_samples)/(NSAMP*stats.norm(0,rad_sigma).pdf(0))\n",
    "        \n",
    "        elif RAD_TYPE == 'rp10':\n",
    "            w_ = stats.norm(rad_center, rad_sigma).pdf(rp10_samples)/(NSAMP*stats.norm(0,rad_sigma).pdf(0))\n",
    "        \n",
    "        elif RAD_TYPE == 'rpadj':\n",
    "            w_ = stats.norm(rad_center, rad_sigma).pdf(rpadj_samples)/(NSAMP*stats.norm(0,rad_sigma).pdf(0))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"RAD_TYPE not implemented\")\n",
    "        \n",
    "        sample_weights[t] = w_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0a4b2-dae5-4ea8-8e13-745a389c8a8d",
   "metadata": {},
   "source": [
    "#### Vectorize samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af7246-aaab-40cb-939d-915aa499f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_array = {}\n",
    "\n",
    "samples_array['ecc']    = np.zeros((len(targets),NSAMP))\n",
    "samples_array['omega']  = np.zeros((len(targets),NSAMP))\n",
    "samples_array['impact'] = np.zeros((len(targets),NSAMP))\n",
    "\n",
    "for i, t in enumerate(targets):\n",
    "    samples_array['ecc'][i]    = np.array(data[t].ECC)\n",
    "    samples_array['omega'][i]  = np.array(data[t].OMEGA)\n",
    "    samples_array['impact'][i] = np.array(data[t].IMPACT)\n",
    "\n",
    "weights = np.zeros(len(targets))\n",
    "\n",
    "for i, t in enumerate(targets):\n",
    "    weights[i] = np.sum(sample_weights[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320c64b",
   "metadata": {},
   "source": [
    "#### Select high weight objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAD_LIM[0] != RAD_LIM[1]:\n",
    "    weights = None\n",
    "\n",
    "else:\n",
    "    w = weights/np.sum(weights)\n",
    "    \n",
    "    order = np.argsort(w)\n",
    "    keep = order[np.cumsum(w[order]) > 0.01]\n",
    "    \n",
    "    for k in samples_array.keys():\n",
    "        samples_array[k] = samples_array[k][keep]\n",
    "\n",
    "    weights = weights[keep]\n",
    "    \n",
    "    print(\"{0} targets have negligible weight within radius bin\".format(len(order)-len(keep)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8cdfdc-5840-446f-b2d9-a0c11007317b",
   "metadata": {},
   "source": [
    "## Run hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running hierarchical MCMC using {0} planets\".format(len(samples_array['ecc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b50187-44bb-4058-a96f-70aa6cff735a",
   "metadata": {},
   "source": [
    "#### Load empirical distribution template (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ad89e-b260-4f91-9d3b-b99c85b98bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISTRIBUTION == 'empirical':\n",
    "    template_values = np.loadtxt(os.path.join(PROJECT_DIR, \"template_distribution.txt\")).T\n",
    "    template_spline = CubicSpline(template_values[0], template_values[1], extrapolate=False)\n",
    "else:\n",
    "    template_values = None\n",
    "    template_spline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87743ebd-c7c1-4d67-af1b-52de5cc70460",
   "metadata": {},
   "source": [
    "#### Build model and run MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BOOTSTRAP == 'none':\n",
    "    use_wlb = False\n",
    "elif BOOTSTRAP == 'wlb':\n",
    "    use_wlb = True\n",
    "elif isinstance(BOOTSTRAP, int):\n",
    "    use_wlb = False\n",
    "else:\n",
    "    raise ValueError(\"BOOTRAP must be 'none', 'wlb', or an integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8365488",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(BOOTSTRAP, int):\n",
    "    ndraw = int(np.ceil(np.sum(weights)))\n",
    "    \n",
    "    trace_list = []\n",
    "    bin_edges_list = []\n",
    "    \n",
    "    for i in range(BOOTSTRAP):\n",
    "        inds = np.random.choice(np.arange(0,len(weights)), size=ndraw, replace=True, p=weights/np.sum(weights))\n",
    "        \n",
    "        \n",
    "        samples = {}\n",
    "        for k in samples_array.keys():\n",
    "            samples[k] = samples_array[k][inds]\n",
    "\n",
    "        # build model and sample from posterior\n",
    "        model, bin_edges = build_simple_model(samples,\n",
    "                                              DISTRIBUTION,\n",
    "                                              NBIN,\n",
    "                                              e_detprior=E_DETPRIOR,\n",
    "                                              b_detprior=B_DETPRIOR,\n",
    "                                              weights=None,\n",
    "                                              use_wlb=False,\n",
    "                                              template_spline=template_spline,\n",
    "                                              eps=1e-6\n",
    "                                             )\n",
    "\n",
    "        with model:\n",
    "            trace = pmx.sample(tune=5000, draws=1000, chains=2, cores=2, target_accept=0.95, return_inferencedata=True)\n",
    "            \n",
    "        trace_list.append(trace)\n",
    "        bin_edges_list.append(bin_edges)\n",
    "     \n",
    "    \n",
    "else:\n",
    "    # build model and sample from posterior\n",
    "    model, bin_edges = build_simple_model(samples_array,\n",
    "                                          DISTRIBUTION,\n",
    "                                          NBIN,\n",
    "                                          e_detprior=E_DETPRIOR,\n",
    "                                          b_detprior=B_DETPRIOR,\n",
    "                                          weights=weights,\n",
    "                                          use_wlb=use_wlb,\n",
    "                                          template_spline=template_spline,\n",
    "                                          eps=1e-6\n",
    "                                         )\n",
    "\n",
    "    with model:\n",
    "        trace = pmx.sample(tune=5000, draws=1000, chains=2, cores=2, target_accept=0.95, return_inferencedata=True)\n",
    "\n",
    "    trace_list = [trace]\n",
    "    bin_edges_list = [bin_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23664aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_PLOT:\n",
    "    ln_pdf = np.percentile(trace.posterior.ln_pdf, [16,50,84], axis=(0,1))\n",
    "    mean_ecc = np.percentile(trace.posterior.mean_ecc, [16,50,84], axis=(0,1))\n",
    "\n",
    "    pdf = np.exp(ln_pdf)\n",
    "    bin_centers = 0.5*(bin_edges[:-1]+bin_edges[1:])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(bin_centers, pdf[1], 'C0', lw=2)\n",
    "    plt.fill_between(bin_centers, pdf[0], pdf[2], color='C0', alpha=0.3)\n",
    "    plt.axvline(mean_ecc[0], ls=':', color='C1')\n",
    "    plt.axvline(mean_ecc[1], ls='-', color='C1')\n",
    "    plt.axvline(mean_ecc[2], ls=':', color='C1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f83d1d-1b03-47af-b94a-62308406220b",
   "metadata": {},
   "source": [
    "## Save posterior trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53ea6f-fea7-4ed4-aaa8-0dcf3257e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, trace in enumerate(trace_list):\n",
    "    bin_edges = bin_edges_list[i]\n",
    "\n",
    "    # refactor dataframe for export\n",
    "    df = trace.to_dataframe(groups='posterior', include_coords=False)\n",
    "\n",
    "    column_map = {}\n",
    "    for k in list(df.keys()):    \n",
    "        if k.find('[') == -1:\n",
    "            column_map[k] = k\n",
    "        else:\n",
    "            column_map[k] = k[:k.find('[')] + '_{:02d}'.format(int(k[k.find('[')+1:-1]))\n",
    "\n",
    "    df = df.rename(columns=column_map)\n",
    "    \n",
    "    # saves as fits HDU\n",
    "    primary_hdu = fits.PrimaryHDU()\n",
    "    header = primary_hdu.header\n",
    "\n",
    "    header['YYYYMMDD'] = YYYYMMDD\n",
    "    header['DIST']     = DISTRIBUTION\n",
    "    header['NSAMP']    = NSAMP\n",
    "    header['NBIN']     = NBIN\n",
    "    header['NOBJ']     = samples_array['ecc'].shape[0]\n",
    "    header['BOOTSTR']  = BOOTSTRAP\n",
    "    header['MULT_0']   = MULTIPLICITY[0]\n",
    "    header['MULT_1']   = MULTIPLICITY[1]\n",
    "    header['PER_0']    = PER_LIM[0]\n",
    "    header['PER_1']    = PER_LIM[1]\n",
    "    header['RAD_TYPE'] = RAD_TYPE\n",
    "    header['RAD_0']    = RAD_LIM[0]\n",
    "    header['RAD_1']    = RAD_LIM[1]\n",
    "    header['RAD_FWHM'] = RAD_FWHM\n",
    "    header['MSTAR_0']  = MSTAR_LIM[0]\n",
    "    header['MSTAR_1']  = MSTAR_LIM[1]\n",
    "    header['RSTAR_0']  = RSTAR_LIM[0]\n",
    "    header['RSTAR_1']  = RSTAR_LIM[1]\n",
    "    header['FEH_0']    = FEH_LIM[0]\n",
    "    header['FEH_1']    = FEH_LIM[1]\n",
    "    header['TEFF_0']   = TEFF_LIM[0]\n",
    "    header['TEFF_1']   = TEFF_LIM[1]\n",
    "    header['AGE_0']    = AGE_LIM[0]\n",
    "    header['AGE_1']    = AGE_LIM[1]\n",
    "    header['E_PRIOR']  = E_DETPRIOR\n",
    "    header['B_PRIOR']  = B_DETPRIOR\n",
    "    header['B_CUT']    = B_CUT\n",
    "    \n",
    "    samples_hdu = fits.BinTableHDU(data=df.to_records(index=False), name='SAMPLES')\n",
    "    binedges_hdu = fits.ImageHDU(bin_edges, name='BINEDGES')\n",
    "\n",
    "    hduL  = fits.HDUList([primary_hdu, samples_hdu, binedges_hdu])\n",
    "    fname = os.path.join(RESULTS_DIR, \"{0}_{1}_{2}.fits\".format(YYYYMMDD, RUN_ID, str(i).zfill(3)))\n",
    "\n",
    "    hduL.writeto(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cd6f9-657d-4b6e-bc79-388befce9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this complicated loading routine deals with big-endian vs little-endian mismatch between pandas and FITS\n",
    "RELOAD = False\n",
    "if RELOAD:\n",
    "    with fits.open(fname) as hduL:\n",
    "        data = hduL['SAMPLES'].data\n",
    "        keys = data.names\n",
    "\n",
    "        _samples = []\n",
    "        for k in keys:\n",
    "            _samples.append(data[k])\n",
    "\n",
    "        samples = pd.DataFrame(np.array(_samples).T, columns=keys)\n",
    "        bin_edges = np.array(hduL['BINEDGES'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b9969",
   "metadata": {},
   "source": [
    "## Exit program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\")\n",
    "print(\"+\"*shutil.get_terminal_size().columns)\n",
    "print(\"Inference complete {0}\".format(datetime.now().strftime(\"%d-%b-%Y at %H:%M:%S\")))\n",
    "print(\"Total runtime = %.1f min\" %((timer()-global_start_time)/60))\n",
    "print(\"+\"*shutil.get_terminal_size().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d64e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
